---
title: "Decision Trees"
author: "Hector Corrada-Bravo and Rafael A. Irizarry"
date: "April 7, 2016"
output: html_document
---

```{r, echo=FALSE}
knitr::opts_chunk$set(cache=TRUE, warning = FALSE, message = FALSE)
```


```{r, echo=FALSE}
library(dplyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
library(readr)
```

# Decision Trees

We have described two type machine learning algorithms. Linear approaches, including linear regression, generalized linear models (GLM), discriminant analysis, and smoothing approaches, including loess and k-nearest neighbors. The linear approaches were limiting in that partition of the prediction space had to be linear (in the case of QDA quadratic). A limitation of the smoothing approach is that with a large number of predictors,  we run into the problem of _the curse of dimensionality_.

## The Curse of Dimensionality

A useful way of understand the curse of dimensionality is by considering how large we have to make a neighborhood/window to include a given percentage of the data. For example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want create windows that include 1/10-th of data. Then it's easy to see that our windows have to be of size 0.1:

```{r, fig.width=10, fig.height=1}
rafalib::mypar()
x <- seq(0,1,len=100)
y <- rep(1, 100)
plot(x,y, xlab="",ylab="", cex=0.25, yaxt="n", xaxt="n",type="n")
lines(x[c(15,35)], y[c(15,35)], col="blue",lwd=3)
points(x,y, cex = 0.25)
points(x[25],y[25],col="blue", cex = 0.5, pch=4)
text(x[c(15,35)], y[c(15,35)], c("[","]"))
```

Now, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point:

```{r, echo=FALSE, fig.width=10, fig.height=10}
tmp <- expand.grid(1:10,1:10)
x <- tmp[,1]
y <- tmp[,2]
plot(x,y, xlab="",ylab="", cex=0.25, yaxt="n", xaxt="n",type="n")
polygon(c(x[25]-0.5, x[25]-0.5, x[25]+0.5, x[25]+0.5),
        c(y[25]-0.5, y[25]+0.5, y[25]+0.5, y[25]-0.5), col="blue")
points(x,y, cex = 0.25)
points(x[25],y[25], cex = 0.5, pch=4)
```

or, if we want to include 10% of the data we need to increase the window size to $\sqrt{10}$:

```{r, echo=FALSE, fig.width=10, fig.height=10}
plot(x,y, xlab="",ylab="", cex=0.25, yaxt="n", xaxt="n",type="n")
polygon(c(x[25]-sqrt(10)/2, x[25]-sqrt(10)/2, x[25]+sqrt(10)/2, x[25]+sqrt(10)/2),
        c(y[25]-sqrt(10)/2, y[25]+sqrt(10)/2, y[25]+sqrt(10)/2, y[25]-sqrt(10)/2),
        col="blue")
points(x,y, cex = 0.25)
points(x[25],y[25], cex = 0.5, pch=4)
```

To include 10% of the data in a case with $p$ features we need an interval for each interval that covers $.10^{1/p}$ of the total. This proportion gets close to 1 (including all the data and no longer smoothing) quickly:

```{r, echo=FALSE}
p <- 1:100
plot(p, .1^(1/p),ylim=c(0,1))
abline(h=1)
```

Here we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes, but still produce models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on Regression and Decision Trees and their extension to Random Forests.

### Regression Trees

Consider the olives dataset below. We show two measured predictors, linoleic and eicosenoic. Suppose we wanted to predict the olive's region using these two predictors. What method would you use?

```{r}
olives <- read.csv("https://raw.githubusercontent.com/datasciencelabs/data/master/olive.csv", as.is=TRUE) %>% tbl_df
names(olives)[1] <- "province"
region_names <- c("Southern Italy","   Sardinia","Northern Italy")
olives <- olives %>% mutate(Region=factor(region_names[Region]))

p <- olives %>% ggplot(aes(eicosenoic, linoleic, fill=Region)) +
  geom_point(pch=21)
p
```

Note that we can describe a classification algorithm that would work pretty much perfectly:

```{r}
p <- p + geom_vline(xintercept = 6.5) + 
  geom_segment(x= -2, y = 1053.5, xend = 6.5, yend = 1053.5)
p
```

The instructions for this prediction approach. The prediction algorithm inferred from the figure above is is what we call a decision tree. If eicosnoic is larger than 6.5 predict Southern Italy. If not, then if linoleic is larger than $1053.5$ predict Sardinia and Norther Italy otherwise. We can draw this decision tree like this:

```{r, echo=FALSE}
library(rpart)

fit <- rpart(as.factor(Region)~., 
             data = select(olives, -province, -Area))
plot(fit)
text(fit, cex = 0.5)
```

Decision trees like this are often used in practice. For example to decide if a person is at risk of having  heart attack doctors use the following:

![](http://www-abc.mpib-berlin.mpg.de/users/ptodd/SimpleHeuristics.BBS/fig1.gif)

The general idea of the methods we are describing is to define an algorithm that uses data to create these tress. 

Regression and decision trees operate by predicting an outcome variable $Y$ by partitioning feature (predictor) space.

## Regression Trees

Let's start with case of a continuous outcome. The general idea here is to build a decision tree and at end of each _node_ we will have a different prediction $\hat{Y}$ for the outcome $Y$..

The regression tree model then:

1. Partitions space into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$.
2. For every observation that falls within region $R_j$, predict response as mean of response for training observations in $R_j$.

The important observation is that **Regression Trees create partition recursively**

For example, consider finding a good predictor $j$ to partition space its axis. A recursive algorithm would look like this:

1. Find predictor $j$ and value $s$ that minimize RSS:

$$
\sum_{i:\, x_i \in R_1(j,s))} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s))} (y_i - \hat{y}_{R_2})^2
$$

Where $R_1$ and $R_2$ are regions resulting from splitting observations on predictor $j$ and value $s$:

$$
R_1(j,s) = \{X|X_j < s\} \mathrm{ and } R_2(j,s) \{X|X_j \geq s\}
$$

This is then applied recursively to regions $R_1$ and $R_2$. Within each region a prediction is made using $\hat{y}_{R_j}$ which is the mean of the response $Y$ of observations in $R_j$.

Let's take a look at what this algorithm does on the 2008 presidential election poll data:

```{r, echo = FALSE}
library(stringr)
library(lubridate)
library(tidyr)
library(XML)
theurl <- paste0("http://www.pollster.com/08USPresGEMvO-2.html")
polls_2008 <- readHTMLTable(theurl,stringsAsFactors=FALSE)[[1]] %>%
  tbl_df() %>%
  separate(col=Dates, into=c("start_date","end_date"), sep="-",fill="right") %>%
  mutate(end_date = ifelse(is.na(end_date), start_date, end_date)) %>%
  separate(start_date, c("smonth", "sday", "syear"), sep = "/",  convert = TRUE, fill = "right")%>%
  mutate(end_date = ifelse(str_count(end_date, "/") == 1, paste(smonth, end_date, sep = "/"), end_date)) %>%
  mutate(end_date = mdy(end_date))  %>% mutate(syear = ifelse(is.na(syear), year(end_date), syear + 2000)) %>%
  unite(start_date, smonth, sday, syear)  %>%
  mutate(start_date = mdy(start_date)) %>%
  separate(`N/Pop`, into=c("N","population_type"), sep="\ ", convert=TRUE, fill="left") %>%
  mutate(Obama = as.numeric(Obama)/100,
         McCain=as.numeric(McCain)/100,
         diff = Obama - McCain,
         day=as.numeric(start_date - mdy("11/04/2008")))
polls_2008 <-  filter(polls_2008, start_date>="2008-06-01") %>%
  group_by(X=day)  %>%
  summarize(Y=mean(diff))

polls_2008 %>% ggplot(aes(X, Y)) + geom_point() 
```

Regression trees are built in R using a similar interface as linear models:

```{r, fig.height=14, fig.height=7}
library(tree)
fit <- tree(Y~X, data = polls_2008)
pred <- predict(fit)
plot(fit)
text(fit, cex = 0.5)
```

With only one feature we can actually plot the prediction:

```{r, fig.height=10, fig.height=7}
polls_2008 %>% mutate(f_hat = pred) %>% ggplot() +
  geom_point(aes(X, Y)) +
  geom_step(aes(X, f_hat), col="blue")
```

The decision trees partitions the `weight` predictor into regions based on its value. We can show this graphically as below. The idea behind the regression tree is that outcome $Y$  is estimated (or predicted) to be it's mean _within each of the data partitions_. Think of it as the conditional mean of $Y$ where conditioning is given by this region partitioning.


### Specifics of the regression tree algorithm

The recursive partitioning algorithm described above leads to a set of natural questions:

_When do we stop partitioning?_ We stop when adding a partition does not reduce RSS, or, when partition has too few training observations. Even then, trees built with this stopping criterion tend to _overfit_ training data. To avoid this, a post-processing step called _pruning_ is used to make the tree smaller.

**Question:** why would a smaller tree tend to generalize better?


The `cv.tree` function is used to determine a reasonable tree depth for the given dataset. For this dataset it seems that a depth of 7 works well:

```{r, echo=TRUE}
fit <- tree(Y~X, data = polls_2008,
            control = tree.control(nobs = nrow(polls_2008), 
                                   mincut = 1, minsize = 2, mindev = 0.001))

cv_polls <- cv.tree(fit)
data_frame(tree_size = cv_polls$size, RSS = cv_polls$dev) %>% 
  filter(tree_size>1 & tree_size < 20) %>%
  ggplot(aes(tree_size, RSS)) + geom_point()
```

Here we see how deep trees can overfit this dataset and perform poorly on new, unseen data. Here is the tree we think will do a good job on prediction based on the plot above.


### Classification (Decision) Trees

Classification, or decision trees, are used in classification problems, where the outcome is categorical. The same partitioning principle, but now, each region predicts the majority class for training observations within region. The recursive partitioning algorithm we saw previously requires a score function to choose predictors (and values) to partition with. In classification we could use a naive approach of looking for partitions that minimize training error. However, better performing approaches use more sophisticated metrics. Here are two of the most popular (denoted for leaf $m$):
 
  - **Gini Index**: $\sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})$, or
  - **Entropy**: $-\sum_{k=1}^K \hat{p}_{mk}\log(\hat{p}_{mk})$
  
where $\hat{p}_{mk}$ is the proportion of training observations in partition $m$ labeled as class $k$. Both of these seek to partition observations into subsets that have the same labels.

Let us look at how a classification tree performs on the digits example we examined before:

```{r, echo=FALSE}
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
if(!exists("digits")) digits <- read_csv(url)
digits <- digits %>% filter(label%in%c(2,7))
digits <- mutate(digits, label =  as.character(label)) %>% 
  mutate(label = ifelse(label=="2",0,1 ))
row_column <- expand.grid(row=1:28, col=1:28)
ind1 <- which(row_column$col <= 14 & row_column$row <=14)
ind2 <- which(row_column$col > 14 & row_column$row > 14)
ind <- c(ind1,ind2)
X <- as.matrix(digits[,-1])
X <- X>200
X1 <- rowSums(X[,ind1])/rowSums(X)
X2 <- rowSums(X[,ind2])/rowSums(X)
digits <- mutate(digits, X_1 = X1, X_2 = X2) %>% select(label, X_1, X_2) %>%
  mutate(label = as.factor(label))
y <- digits$label
x <- cbind(X1, X2)
library(caret)
fit <- knn3(x, y, 51)
GS <- 150
X1s <- seq(min(X1),max(X1),len=GS)
X2s <- seq(min(X2),max(X2),len=GS)
true_f <- expand.grid(X_1=X1s, X_2=X2s)
yhat <- predict(fit, newdata = true_f, type="prob")[,2]
true_f <- mutate(true_f, yhat=yhat)
f <- loess(yhat~X_1*X_2, data=true_f, 
           degree=1, span=1/5)$fitted
true_f <- true_f %>% mutate(f=f) 
rm(X,X1,X2,fit,GS,X1s,X2s,yhat,f)

true_f_plot <- true_f %>%
  ggplot(aes(X_1, X_2, fill=f))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + geom_raster()  + guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f), data=true_f, breaks=c(0.5),color="black",lwd=1.5)
true_f_plot
```


```{r, fig.width=14, fig.height=6, echo = FALSE}
inTrain <- createDataPartition(y = digits$label, p=0.5)
digits_train <- slice(digits, inTrain$Resample1)
digits_test <- slice(digits, -inTrain$Resample1)

fit <- tree(label~X_1+X_2, data=digits_train)
plot(fit)
text(fit)
```

We can see the prediction here:

```{r}
f_hat_cart <- predict(fit, newdata = true_f)[,2]

p <-true_f %>% mutate(f=f_hat_cart) %>%
 ggplot(aes(X_1, X_2, fill=f))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + 
  geom_raster()  + #guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f), 
               data=mutate(true_f, f=f_hat_cart),
               breaks=c(0.5),color="black",lwd=1.5) +
  guides(fill = FALSE)

library(gridExtra)
grid.arrange(true_f_plot, p, nrow=1)
```

To prune the tree we can use the `prune.tree` function. In this case the pruned tree does not differ much fro the original.

```{r}
pruned_fit <- prune.tree(fit)
plot(pruned_fit)
```

Here is what a pruned tree looks like:

```{r}
pruned_fit <- prune.tree(fit, k = 160)
f_hat_cart2 <- predict(pruned_fit, newdata = true_f)[,2]
p <-true_f %>% mutate(f=f_hat_cart2) %>%
 ggplot(aes(X_1, X_2, fill=f))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + 
  geom_raster()  + #guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f), 
               data=mutate(true_f, f=f_hat_cart2),
               breaks=c(0.5),color="black",lwd=1.5)
p
```



Classification trees have certain advantages that make them very useful. They are
highly interpretable, even more so than linear models. Are easy to visualize (if small enough), they (maybe) model human decision processes and don't require that dummy predictors for categorical variables are used.

On the other hand, the greedy approach via recursive partitioning is a bit harder to train than linear regression. It may not always be the best performing method since it is not very flexible and are highly unstable to changes in training data.

Below we will learn about the bootstrap 

## Bootstrap

Suppose the income distribution of your population is as follows:

```{r, echo = FALSE}
n <- 10^6
income <- 10^(rnorm(n, 4.656786, 0.4394738))
```

```{r}
hist(log10(income))
```

The population median is 
```{r}
m <- median(income)
m
```

Suppose we don't have access to the entire population but want to estimate the median $m$. We take a sample of 250 and estimate the population median $m$ with the sample median $M$:

```{r}
set.seed(1)
N <- 250
X <- sample(income, N)
M <- median(X)
M
```

Can we construct a confidence interval? What is the distribution of $M$ ?

From a Monte Carlo simulation we see that the distribution of $M$ is approximately normal with the following expected value and standard error:

```{r}
B <- 10^5
Ms <- replicate(B, {
  X <- sample(income, N)
  M <- median(X)
})
par(mfrow=c(1,2))
hist(Ms)
qqnorm(Ms)
qqline(Ms)
mean(Ms)
sd(Ms)
```

The problem here is that, as we have described before, in practice we do not have access to the distribution. In the past we have used the central limit theorem. But the CLT we studies applies to averages and here we are interested in the median. 

The Bootstrap permits us to approximate a Monte Carlo simulation without access to the entire distribution. The general idea is relatively simple. We act as if the sample is the distribution and sample (with replacement) datasets of the same size. Then we compute the summary statistic, in this case median, on this _bootstrap sample_. 

There is theory telling us that the distribution of the statistics obtained with bootstrap samples approximate the distribution of our actual statistic. This is how we construct bootstrap samples and an approximate distribution:


```{r}
B <- 10^5
M_stars <- replicate(B, {
  X_star <- sample(X, N, replace = TRUE)
  M_star <- median(X_star)
})
```

Now we can check how close it is to the actual distribution
```{r}
qqplot(Ms, M_stars)
abline(0,1)  
```

We see it is not perfect but it provides a decent approximation:

```{r}
quantile(Ms, c(0.05, 0.95))
quantile(M_stars, c(0.05, 0.95))
```

This is much better than what we get if we mindlessly use the CLT:
```{r}
median(X) + 1.96 * sd(X)/sqrt(N) * c(-1,1)
```


If we know the distribution is normal, we can use the bootstrap to estimate the mean:
```{r}
mean(Ms) + 1.96*sd(Ms)*c(-1,1)
mean(M_stars) + 1.96*sd(M_stars)*c(-1,1)
```



## Random Forrests

Random Forests are a **very popular** approach that address the shortcomings of decision trees via re-sampling of the training data. Their goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest constructed with randomness). It has to features that help accomplish this.

The first trick is *Bagging* (bootstrap aggregation)
General scheme:
  1. Build many decision trees $T_1, T_2, \ldots, T_B$ from training set
  2. Given a new observation, let each $T_j$ predict $\hat{y}_j$
  3. For regression: predict average $\frac{1}{B} \sum_{j=1}^B \hat{y}_j$,
     for classification: predict with majority vote (most frequent class)
     
But how do we get many decision trees from a single training set?

For this we use the _bootstrap_. To create $T_j, \, j=1,\ldots,B$ from training set of size $N$:

a) create a bootstrap training set by sampling $N$ observations from training set **with replacement**
b) build a decision tree from bootstrap training set

Here is the Random Forest estimate of the 2008 polls data:

```{r}
library(randomForest)

fit <- randomForest(Y~X, data = polls_2008, ntree=500, nodesize = 15, mtry=1)
pred <- predict(fit, newdata = polls_2008)

polls_2008 %>%
  mutate(f_hat = pred) %>% ggplot() +
  geom_point(aes(X, Y)) +
  geom_line(aes(X, f_hat), col="blue")
```

The averaging is what permits estimates that are not step functions.
The following animation helps illustrate this procedure.

```{r}
if(!file.exists("rf.gif")){
library(rafalib)
set.seed(1)
ntrees <- 50
sum <- rep(0,nrow(polls_2008))
res <- vector("list", ntrees)
XLIM <- range(polls_2008$X)
YLIM <- range(polls_2008$Y)
path <- tempdir()
for(i in 0:ntrees){
  png(file.path(path,sprintf("plot%02d.png",i)), width = 480, height = 350)
  mypar(1,1)
  if(i==0){
    with(polls_2008, plot(X, Y, pch = 1, main="Data", xlim=XLIM,
                          ylim=YLIM,
                          xlab = "Days", ylab="Obama - McCain"))
  } else{
    ind <- sort(sample(1:nrow(polls_2008), replace = TRUE))
    tmp <- polls_2008[ind,]
    fit <- rpart(Y~X, data = tmp)
    pred <- predict(fit, newdata = tmp)
    res[[i]] <- data_frame(X = tmp$X, Y=pred)
    pred <- predict(fit, newdata = polls_2008)
    sum <- sum+pred
    avg <- sum/i
    with(tmp, plot(X,Y, pch=1, xlim=XLIM, ylim=YLIM, type="n",
                   xlab = "Days", ylab="Obama - McCain",
                   main=ifelse(i==1, paste(i, "tree"),paste(i, "trees"))))
    for(j in 1:i){
      with(res[[j]], lines(X, Y, type="s", col="grey", lty=2))
    }
    with(tmp, points(X,Y, pch=1))
    with(res[[i]], lines(X, Y, type="s",col="azure4",lwd=2))
    lines(polls_2008$X, avg, lwd=3, col="blue")
  }
  dev.off()
}
for(i in 1:5){
  png(file.path(path,sprintf("plot%2d.png",ntrees+i)), width = 480, height = 350)
  mypar(1,1)
  with(polls_2008, plot(X, Y, pch = 1, main="Final", xlim=XLIM, ylim=YLIM,
                          xlab = "Days", ylab="Obama - McCain"))
  lines(polls_2008$X, avg, lwd=3, col="blue")
  dev.off()
}
system(paste0("cd ",path,";convert -loop 0 -delay 50 *.png rf.gif; mv rf.gif ",getwd(),"/"))
}
```

![](rf.gif)

The second Random Forests feature is to use a random selection of features to split when deciding partitions. Specifically, when building each tree $T_j$, at each recursive partition only consider a randomly selected subset of predictors to check for best split. This reduces correlation between trees in forest, improving prediction accuracy.

Here is the random forest fit for the digits data:

```{r}
library(randomForest)
fit <- randomForest(label~X_1+X_2, data=digits_train)
f_hat_rf <- predict(fit, newdata = true_f, type="prob")[,2]

p <-true_f %>% mutate(f=f_hat_rf) %>%
 ggplot(aes(X_1, X_2, fill=f))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + 
  geom_raster()  + #guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f), 
               data=mutate(true_f, f=f_hat_rf),
               breaks=c(0.5),color="black",lwd=1.5) +
  guides(fill = FALSE)

library(gridExtra)
grid.arrange(true_f_plot, p, nrow=1)
```

We can control the "smoothness" of the random forest estimate in several ways. One is to limit the size of each node. We can require the number of points per node to be larger:

```{r}
## smoother version
fit <- randomForest(as.factor(label)~X_1+X_2,
                    nodesize = 250,
                    data=digits_train)
f_hat_rf <- predict(fit, newdata = true_f, type="prob")[,2]

p <-true_f %>% mutate(f=f_hat_rf) %>%
 ggplot(aes(X_1, X_2, fill=f))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + 
  geom_raster()  + #guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f), 
               data=mutate(true_f, f=f_hat_rf),
               breaks=c(0.5),color="black",lwd=1.5) +
  guides(fill = FALSE)

library(gridExtra)
p
```


We can compare the results:

```{r}
library(caret)
get_accuracy <- function(fit){
  pred <- predict(fit, newdata = digits_test, type = "class")
  confusionMatrix(table(pred = pred, true = digits_test$label))$overall[1]
}
fit <- tree(label~X_1+X_2, data=digits_train)
get_accuracy(fit)

fit <- randomForest(label~X_1+X_2, data=digits_train)
get_accuracy(fit)

fit <- randomForest(label~X_1+X_2,
                    nodesize = 250,
                    data=digits_train)
get_accuracy(fit)
```





A disadvantage of random forests is that we lose interpretability. However, we can use the fact that a bootstrap sample was used to construct trees to measure _variable importance_ from the random forest.

Let's see this using all the digits data

```{r}
library(readr)
library(dplyr)
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
digits <- read_csv(url)

digits <- mutate(digits, label = as.factor(label))
library(caret)
set.seed(1)
inTrain <- createDataPartition(y = digits$label, p=0.9)
train_set <- slice(digits, inTrain$Resample1)
test_set <- slice(digits, -inTrain$Resample1)

fit <- randomForest(label~., ntree = 100, data = train_set)#, do.trace = TRUE)
```

How well does it do?

```{r}
pred <- predict(fit, newdata = test_set, type = "class")
  confusionMatrix(table(pred = pred, true = test_set$label))
```

Here is a table of _variable importance_ for the random forest we just constructed.


```{r, echo=TRUE, results="asis"}
library(knitr)
variable_importance <- importance(fit) 
tmp <- data_frame(feature = rownames(variable_importance),
                  Gini = variable_importance[,1]) %>%
  arrange(desc(Gini))
kable(tmp[1:10,])
```

We can see where the "important" feature are:

```{r}
expand.grid(Row=1:28, Column=1:28) %>%
      mutate(value = variable_importance[,1]) %>%
  ggplot(aes(Row, Column, fill=value)) +
  geom_raster() +
  scale_y_reverse() 
```


And a barplot of the same data.

```{r}
tmp %>% filter(Gini > 200) %>%
  ggplot(aes(x=reorder(feature, Gini), y=Gini)) +
  geom_bar(stat='identity') +
  coord_flip() + xlab("Feature") +
  theme(axis.text=element_text(size=8))
```

### Tree-based methods summary

Tree-based methods are very interpretable _prediction_ models. For which some inferential tasks are possible (e.g., variable importance in random forests), but are much more limited than the linear models we saw previously. These methods are very commonly used across many application domains and Random Forests often perform at state-of-the-art for many tasks.













<!-- ```{r, eval=FALSE, echo=FALSE} -->
<!-- ## Example using gganimate -->
<!-- ntrees <- 10 -->
<!-- avg <- rep(0,nrow(polls_2008)) -->
<!-- res <- vector("list", ntrees) -->
<!-- avg_tab <- vector("list", ntrees) -->
<!-- for(i in 1:ntrees){ -->
<!--   ind <- sort(sample(1:nrow(polls_2008), replace = TRUE)) -->
<!--   tmp <- polls_2008[ind,] -->
<!--   fit <- rpart(Y~X, data = tmp) -->
<!--   pred <- predict(fit, newdata = tmp) -->
<!--   res[[i]] <- data_frame(tree = i, X = tmp$X, Y=tmp$Y, pred) -->
<!--   pred <- predict(fit, newdata = polls_2008) -->
<!--   avg <- avg+pred -->
<!--   avg_tab[[i]] <- data_frame(tree=i, X=polls_2008$X, avg = avg/i) -->
<!-- } -->
<!-- res <- Reduce(rbind, res) -->
<!-- avg <- Reduce(rbind, avg_tab) -->

<!-- p <- ggplot() + geom_point(aes(X,Y,frame=tree), data = res) +  -->
<!--   geom_step(aes(X, pred, frame=tree), data = res, col="darkgrey", lwd=1.5) + -->
<!--   geom_line(aes(X, avg, frame=tree), data = avg, col="blue")  -->


<!-- ##  geom_step(aes(X, pred, frame=tree, cumulative=TRUE,  col=tree),data = res)  -->

<!-- library(gganimate) -->
<!-- gg_animate(p, interval = 0.45) -->
<!-- ``` -->


